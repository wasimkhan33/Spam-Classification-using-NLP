{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasimkhan33/Spam-Classification-using-NLP/blob/main/spam_classifier_using_basic_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6560381",
      "metadata": {
        "papermill": {
          "duration": 0.023062,
          "end_time": "2021-09-23T17:07:10.170610",
          "exception": false,
          "start_time": "2021-09-23T17:07:10.147548",
          "status": "completed"
        },
        "tags": [],
        "id": "c6560381"
      },
      "source": [
        "# Introduction to NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe413ea",
      "metadata": {
        "papermill": {
          "duration": 0.021532,
          "end_time": "2021-09-23T17:07:10.214337",
          "exception": false,
          "start_time": "2021-09-23T17:07:10.192805",
          "status": "completed"
        },
        "tags": [],
        "id": "1fe413ea"
      },
      "source": [
        "Natural Language Processing(NLP) is a subset of Artifical Intelligence(AI) that helps the computers to understand, interpret and utilize the human language. NLP allows the applications to communicate with people using human language.\n",
        "Whenever our data contains large chunks of text, we use NLP techniques to first clean that data and then feed it to the model. We will understand each and every step in detail one by one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0f3493",
      "metadata": {
        "papermill": {
          "duration": 0.027041,
          "end_time": "2021-09-23T17:07:10.263197",
          "exception": false,
          "start_time": "2021-09-23T17:07:10.236156",
          "status": "completed"
        },
        "tags": [],
        "id": "1b0f3493"
      },
      "source": [
        "**Steps in any basic NLP project**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c843a81e",
      "metadata": {
        "papermill": {
          "duration": 0.023154,
          "end_time": "2021-09-23T17:07:10.311450",
          "exception": false,
          "start_time": "2021-09-23T17:07:10.288296",
          "status": "completed"
        },
        "tags": [],
        "id": "c843a81e"
      },
      "source": [
        "1. Tokenization (Breaking down of large texts into smaller tokens i.e. paragraphs to sentences and sentences to words)\n",
        "2. Text Data Cleaning (Removing punctuations, stopwords, converting to lower cases etc)\n",
        "3. Stemming or Lemmatization (Removing the suffixes of similar words to their root word to get uniformity)\n",
        "4. Converting the remaining words to vectors by pre-processing techniques like Bag of Words.\n",
        "5. Feeding those vectors to the model\n",
        "\n",
        "In this kernel, let's go through all these points by implementing a basic Spam classifier using NLP techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5245a4c9",
      "metadata": {
        "papermill": {
          "duration": 0.024179,
          "end_time": "2021-09-23T17:07:10.358512",
          "exception": false,
          "start_time": "2021-09-23T17:07:10.334333",
          "status": "completed"
        },
        "tags": [],
        "id": "5245a4c9"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e93799c1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:10.414573Z",
          "iopub.status.busy": "2021-09-23T17:07:10.413901Z",
          "iopub.status.idle": "2021-09-23T17:07:11.306622Z",
          "shell.execute_reply": "2021-09-23T17:07:11.305197Z",
          "shell.execute_reply.started": "2021-09-23T15:53:53.159439Z"
        },
        "papermill": {
          "duration": 0.925102,
          "end_time": "2021-09-23T17:07:11.306834",
          "exception": false,
          "start_time": "2021-09-23T17:07:10.381732",
          "status": "completed"
        },
        "tags": [],
        "id": "e93799c1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81c2c2d",
      "metadata": {
        "papermill": {
          "duration": 0.021515,
          "end_time": "2021-09-23T17:07:11.350442",
          "exception": false,
          "start_time": "2021-09-23T17:07:11.328927",
          "status": "completed"
        },
        "tags": [],
        "id": "b81c2c2d"
      },
      "source": [
        "**Importing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c574f885",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:11.399926Z",
          "iopub.status.busy": "2021-09-23T17:07:11.399221Z",
          "iopub.status.idle": "2021-09-23T17:07:11.433521Z",
          "shell.execute_reply": "2021-09-23T17:07:11.432312Z",
          "shell.execute_reply.started": "2021-09-23T16:03:37.477220Z"
        },
        "papermill": {
          "duration": 0.061404,
          "end_time": "2021-09-23T17:07:11.433711",
          "exception": false,
          "start_time": "2021-09-23T17:07:11.372307",
          "status": "completed"
        },
        "tags": [],
        "id": "c574f885"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('../input/sms-spam-collection-dataset/spam.csv',encoding=('ISO-8859-1'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e8c6305",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:11.486285Z",
          "iopub.status.busy": "2021-09-23T17:07:11.485371Z",
          "iopub.status.idle": "2021-09-23T17:07:11.499526Z",
          "shell.execute_reply": "2021-09-23T17:07:11.500047Z",
          "shell.execute_reply.started": "2021-09-23T16:03:45.905083Z"
        },
        "papermill": {
          "duration": 0.044468,
          "end_time": "2021-09-23T17:07:11.500212",
          "exception": false,
          "start_time": "2021-09-23T17:07:11.455744",
          "status": "completed"
        },
        "tags": [],
        "id": "4e8c6305",
        "outputId": "2085f026-a1dc-4783-ff5a-1d21fa08f7d1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     v1                                                 v2 Unnamed: 2  \\\n",
              "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
              "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
              "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
              "\n",
              "  Unnamed: 3 Unnamed: 4  \n",
              "0        NaN        NaN  \n",
              "1        NaN        NaN  \n",
              "2        NaN        NaN  \n",
              "3        NaN        NaN  \n",
              "4        NaN        NaN  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41282e77",
      "metadata": {
        "papermill": {
          "duration": 0.022019,
          "end_time": "2021-09-23T17:07:11.545213",
          "exception": false,
          "start_time": "2021-09-23T17:07:11.523194",
          "status": "completed"
        },
        "tags": [],
        "id": "41282e77"
      },
      "source": [
        "First let's drop the last three columns as they are of no use. v1 column is our target variable(predict whether spam or ham) and v2 column is independent variable. Also, we will rename v1 and v2 columns to label and message respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "237b6220",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:11.593782Z",
          "iopub.status.busy": "2021-09-23T17:07:11.592800Z",
          "iopub.status.idle": "2021-09-23T17:07:11.606517Z",
          "shell.execute_reply": "2021-09-23T17:07:11.605938Z",
          "shell.execute_reply.started": "2021-09-23T16:18:29.486005Z"
        },
        "papermill": {
          "duration": 0.038835,
          "end_time": "2021-09-23T17:07:11.606674",
          "exception": false,
          "start_time": "2021-09-23T17:07:11.567839",
          "status": "completed"
        },
        "tags": [],
        "id": "237b6220"
      },
      "outputs": [],
      "source": [
        "df=df.drop('Unnamed: 2',axis=1)\n",
        "df=df.drop('Unnamed: 3',axis=1)\n",
        "df=df.drop('Unnamed: 4',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d558e5f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:11.660778Z",
          "iopub.status.busy": "2021-09-23T17:07:11.659958Z",
          "iopub.status.idle": "2021-09-23T17:07:11.663208Z",
          "shell.execute_reply": "2021-09-23T17:07:11.663701Z",
          "shell.execute_reply.started": "2021-09-23T16:20:03.284347Z"
        },
        "papermill": {
          "duration": 0.034566,
          "end_time": "2021-09-23T17:07:11.663864",
          "exception": false,
          "start_time": "2021-09-23T17:07:11.629298",
          "status": "completed"
        },
        "tags": [],
        "id": "8d558e5f",
        "outputId": "f77e06ba-96eb-4703-c8a5-a3f9ff6a1350"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     v1                                                 v2\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "872dfd07",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:11.720126Z",
          "iopub.status.busy": "2021-09-23T17:07:11.719229Z",
          "iopub.status.idle": "2021-09-23T17:07:11.722576Z",
          "shell.execute_reply": "2021-09-23T17:07:11.723106Z",
          "shell.execute_reply.started": "2021-09-23T16:20:33.961980Z"
        },
        "papermill": {
          "duration": 0.036795,
          "end_time": "2021-09-23T17:07:11.723282",
          "exception": false,
          "start_time": "2021-09-23T17:07:11.686487",
          "status": "completed"
        },
        "tags": [],
        "id": "872dfd07",
        "outputId": "2b5629f0-2afb-4c36-d2e9-118aa403d053"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=df.rename(columns={'v1':'label','v2':'message'})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1ac37da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:11.775571Z",
          "iopub.status.busy": "2021-09-23T17:07:11.774705Z",
          "iopub.status.idle": "2021-09-23T17:07:12.608215Z",
          "shell.execute_reply": "2021-09-23T17:07:12.608875Z",
          "shell.execute_reply.started": "2021-09-23T16:45:45.405357Z"
        },
        "papermill": {
          "duration": 0.862385,
          "end_time": "2021-09-23T17:07:12.609076",
          "exception": false,
          "start_time": "2021-09-23T17:07:11.746691",
          "status": "completed"
        },
        "tags": [],
        "id": "d1ac37da"
      },
      "outputs": [],
      "source": [
        "import re              #importing necessary NLP libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c624b81",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:12.661565Z",
          "iopub.status.busy": "2021-09-23T17:07:12.660923Z",
          "iopub.status.idle": "2021-09-23T17:07:12.663664Z",
          "shell.execute_reply": "2021-09-23T17:07:12.663053Z",
          "shell.execute_reply.started": "2021-09-23T16:46:03.176991Z"
        },
        "papermill": {
          "duration": 0.031268,
          "end_time": "2021-09-23T17:07:12.663813",
          "exception": false,
          "start_time": "2021-09-23T17:07:12.632545",
          "status": "completed"
        },
        "tags": [],
        "id": "6c624b81"
      },
      "outputs": [],
      "source": [
        "lemmatizer=WordNetLemmatizer()              #object creation for lemmatzation on corpus of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f66ccf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:12.713814Z",
          "iopub.status.busy": "2021-09-23T17:07:12.713140Z",
          "iopub.status.idle": "2021-09-23T17:07:12.717725Z",
          "shell.execute_reply": "2021-09-23T17:07:12.717058Z",
          "shell.execute_reply.started": "2021-09-23T16:51:00.481678Z"
        },
        "papermill": {
          "duration": 0.030718,
          "end_time": "2021-09-23T17:07:12.717873",
          "exception": false,
          "start_time": "2021-09-23T17:07:12.687155",
          "status": "completed"
        },
        "tags": [],
        "id": "73f66ccf"
      },
      "outputs": [],
      "source": [
        "corpus=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "910680cc",
      "metadata": {
        "papermill": {
          "duration": 0.023574,
          "end_time": "2021-09-23T17:07:12.765490",
          "exception": false,
          "start_time": "2021-09-23T17:07:12.741916",
          "status": "completed"
        },
        "tags": [],
        "id": "910680cc"
      },
      "source": [
        "Next step is to remove the punctuations, stopwords like 'a', 'the', 'is' etc as these words do not contribute to the model.\n",
        "Also, convert into lower cases as similar words with different cases will be treated differently.\n",
        "Then apply Lemmatization on those texts(each row) to remove the suffixes and reduce it to its dictionary root form.\n",
        "Words like studies, studying will get converted to study. This is done to bring uniformity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a287e9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:12.823187Z",
          "iopub.status.busy": "2021-09-23T17:07:12.817511Z",
          "iopub.status.idle": "2021-09-23T17:07:15.871635Z",
          "shell.execute_reply": "2021-09-23T17:07:15.871051Z",
          "shell.execute_reply.started": "2021-09-23T16:51:07.082666Z"
        },
        "papermill": {
          "duration": 3.082388,
          "end_time": "2021-09-23T17:07:15.871842",
          "exception": false,
          "start_time": "2021-09-23T17:07:12.789454",
          "status": "completed"
        },
        "tags": [],
        "id": "f3a287e9"
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(df)):\n",
        "    review=re.sub('[^a-zA-Z]','',df['message'][i])\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    review=[lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
        "    review=' '.join(review)\n",
        "    corpus.append(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cbdca05",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:15.927812Z",
          "iopub.status.busy": "2021-09-23T17:07:15.926970Z",
          "iopub.status.idle": "2021-09-23T17:07:15.930511Z",
          "shell.execute_reply": "2021-09-23T17:07:15.931103Z",
          "shell.execute_reply.started": "2021-09-23T16:53:11.614289Z"
        },
        "papermill": {
          "duration": 0.036146,
          "end_time": "2021-09-23T17:07:15.931278",
          "exception": false,
          "start_time": "2021-09-23T17:07:15.895132",
          "status": "completed"
        },
        "tags": [],
        "id": "5cbdca05",
        "outputId": "e30a0b12-1cf7-442c-bfa9-22692a090fa2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71199248",
      "metadata": {
        "papermill": {
          "duration": 0.023818,
          "end_time": "2021-09-23T17:07:15.979283",
          "exception": false,
          "start_time": "2021-09-23T17:07:15.955465",
          "status": "completed"
        },
        "tags": [],
        "id": "71199248"
      },
      "source": [
        "Now that we have cleaned the data, its time to convert those words into vectors using Bag of Words technique.\n",
        "\n",
        "In this technique I have explained the Bag of words in detail. You can refer the logic of technique from there.\n",
        "In short Bag of words just creates a set of vectors containing count of word occurences in the document and it creates such vectors for each row of message column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20b8d41a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.031369Z",
          "iopub.status.busy": "2021-09-23T17:07:16.030619Z",
          "iopub.status.idle": "2021-09-23T17:07:16.033899Z",
          "shell.execute_reply": "2021-09-23T17:07:16.034444Z",
          "shell.execute_reply.started": "2021-09-23T16:59:04.759430Z"
        },
        "papermill": {
          "duration": 0.031342,
          "end_time": "2021-09-23T17:07:16.034621",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.003279",
          "status": "completed"
        },
        "tags": [],
        "id": "20b8d41a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e70d3b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.085892Z",
          "iopub.status.busy": "2021-09-23T17:07:16.085267Z",
          "iopub.status.idle": "2021-09-23T17:07:16.089068Z",
          "shell.execute_reply": "2021-09-23T17:07:16.089555Z",
          "shell.execute_reply.started": "2021-09-23T16:59:42.826471Z"
        },
        "papermill": {
          "duration": 0.031294,
          "end_time": "2021-09-23T17:07:16.089790",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.058496",
          "status": "completed"
        },
        "tags": [],
        "id": "01e70d3b"
      },
      "outputs": [],
      "source": [
        "cv=CountVectorizer(max_features=1000)         #countvectorizer is the library which helps in creating BOW. cv is the object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b5e2be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.167857Z",
          "iopub.status.busy": "2021-09-23T17:07:16.157428Z",
          "iopub.status.idle": "2021-09-23T17:07:16.194933Z",
          "shell.execute_reply": "2021-09-23T17:07:16.194254Z",
          "shell.execute_reply.started": "2021-09-23T17:00:33.130463Z"
        },
        "papermill": {
          "duration": 0.080572,
          "end_time": "2021-09-23T17:07:16.195100",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.114528",
          "status": "completed"
        },
        "tags": [],
        "id": "22b5e2be"
      },
      "outputs": [],
      "source": [
        "X=cv.fit_transform(corpus).toarray()            #independent variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "870f973e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.249091Z",
          "iopub.status.busy": "2021-09-23T17:07:16.248302Z",
          "iopub.status.idle": "2021-09-23T17:07:16.253273Z",
          "shell.execute_reply": "2021-09-23T17:07:16.253731Z",
          "shell.execute_reply.started": "2021-09-23T17:00:49.007022Z"
        },
        "papermill": {
          "duration": 0.034724,
          "end_time": "2021-09-23T17:07:16.253905",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.219181",
          "status": "completed"
        },
        "tags": [],
        "id": "870f973e"
      },
      "outputs": [],
      "source": [
        "y=pd.get_dummies(df['label'])               #creating one hot(dummy variables) vectors for target variable\n",
        "y=y.iloc[:,1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fdd56ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.308845Z",
          "iopub.status.busy": "2021-09-23T17:07:16.308119Z",
          "iopub.status.idle": "2021-09-23T17:07:16.341134Z",
          "shell.execute_reply": "2021-09-23T17:07:16.340548Z",
          "shell.execute_reply.started": "2021-09-23T17:01:33.415950Z"
        },
        "papermill": {
          "duration": 0.06369,
          "end_time": "2021-09-23T17:07:16.341307",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.277617",
          "status": "completed"
        },
        "tags": [],
        "id": "0fdd56ff"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split                      #Do the train test split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6878ec0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.393763Z",
          "iopub.status.busy": "2021-09-23T17:07:16.393101Z",
          "iopub.status.idle": "2021-09-23T17:07:16.397846Z",
          "shell.execute_reply": "2021-09-23T17:07:16.397299Z",
          "shell.execute_reply.started": "2021-09-23T17:01:52.046245Z"
        },
        "papermill": {
          "duration": 0.032898,
          "end_time": "2021-09-23T17:07:16.397987",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.365089",
          "status": "completed"
        },
        "tags": [],
        "id": "e6878ec0"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB          #for this we will use Multinomial Bayes algorithm to determine the label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b57762",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.450307Z",
          "iopub.status.busy": "2021-09-23T17:07:16.449329Z",
          "iopub.status.idle": "2021-09-23T17:07:16.482993Z",
          "shell.execute_reply": "2021-09-23T17:07:16.482356Z",
          "shell.execute_reply.started": "2021-09-23T17:02:05.523857Z"
        },
        "papermill": {
          "duration": 0.061467,
          "end_time": "2021-09-23T17:07:16.483146",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.421679",
          "status": "completed"
        },
        "tags": [],
        "id": "76b57762"
      },
      "outputs": [],
      "source": [
        "spam_detect_model=MultinomialNB().fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f77f1aa6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.537156Z",
          "iopub.status.busy": "2021-09-23T17:07:16.535927Z",
          "iopub.status.idle": "2021-09-23T17:07:16.553023Z",
          "shell.execute_reply": "2021-09-23T17:07:16.552314Z",
          "shell.execute_reply.started": "2021-09-23T17:02:49.021667Z"
        },
        "papermill": {
          "duration": 0.046261,
          "end_time": "2021-09-23T17:07:16.553176",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.506915",
          "status": "completed"
        },
        "tags": [],
        "id": "f77f1aa6"
      },
      "outputs": [],
      "source": [
        "y_pred=spam_detect_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5af9b5b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.643293Z",
          "iopub.status.busy": "2021-09-23T17:07:16.642293Z",
          "iopub.status.idle": "2021-09-23T17:07:16.644506Z",
          "shell.execute_reply": "2021-09-23T17:07:16.645628Z",
          "shell.execute_reply.started": "2021-09-23T17:05:04.978376Z"
        },
        "papermill": {
          "duration": 0.050897,
          "end_time": "2021-09-23T17:07:16.645918",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.595021",
          "status": "completed"
        },
        "tags": [],
        "id": "d5af9b5b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bbd039",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-23T17:07:16.709696Z",
          "iopub.status.busy": "2021-09-23T17:07:16.708806Z",
          "iopub.status.idle": "2021-09-23T17:07:16.711810Z",
          "shell.execute_reply": "2021-09-23T17:07:16.712298Z",
          "shell.execute_reply.started": "2021-09-23T17:05:12.566994Z"
        },
        "papermill": {
          "duration": 0.032839,
          "end_time": "2021-09-23T17:07:16.712461",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.679622",
          "status": "completed"
        },
        "tags": [],
        "id": "33bbd039",
        "outputId": "fcf6d41c-bd2b-413d-f410-7100be16866e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8556053811659193\n"
          ]
        }
      ],
      "source": [
        "print(accuracy_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b96e609a",
      "metadata": {
        "papermill": {
          "duration": 0.023847,
          "end_time": "2021-09-23T17:07:16.760624",
          "exception": false,
          "start_time": "2021-09-23T17:07:16.736777",
          "status": "completed"
        },
        "tags": [],
        "id": "b96e609a"
      },
      "source": [
        "This was all about the basic flow of a NLP project through spam classifier project. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 15.752003,
      "end_time": "2021-09-23T17:07:18.139109",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-23T17:07:02.387106",
      "version": "2.3.3"
    },
    "colab": {
      "name": "spam-classifier-using-basic-nlp.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}